{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reset -f"
      ],
      "metadata": {
        "id": "IbFrtuoVHlhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIPxX-6ndmlB"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece datasets evaluate accelerate transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dZbSPi6tgE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29718cef-6620-4146-a1c5-12f51e86e758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline, ViTImageProcessor, ViTForImageClassification, ViTModel, AutoTokenizer, T5Tokenizer, MT5Model, AutoFeatureExtractor, MT5ForConditionalGeneration\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset, load_from_disk\n",
        "import evaluate\n",
        "import gc\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Y9Ue2j8PHXRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml\n",
        "from pynvml import *\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kid2fWd29f6K",
        "outputId": "4d95e9dc-cc40-457b-9373-4ee4e6a3a63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pynvml in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (11.5.0)\n",
            "GPU memory occupied: 2792 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmihHL2gyfLA"
      },
      "source": [
        "We want to use a ViT + mT5 model to work try mulitlingual VQA. ViT is a vision model that has an encoder transformer model like BERT. mT5 is a multilingual seq to seq transformer model (encoder - decoder).\n",
        "\n",
        "To combine these two models, need to introduce the visual information into the question.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LmEH4UudOOl"
      },
      "outputs": [],
      "source": [
        "#annot_path = '/content/drive/Shareddrives/CS263_final/models/data/evjvqa_train.json'\n",
        "#img_zip_path = '/content/drive/Shareddrives/CS263_final/models/data/train-images.zip'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCxSG7viBduy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getcwd() == '/content':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  train_val_dataset = load_from_disk('/content/drive/Shareddrives/CS263_final/models/data/evjvqa_train_PIL_image')\n",
        "else:\n",
        "  train_val_dataset = load_from_disk('G:/Shared drives/CS263_final/models/data/evjvqa_train_PIL_image')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "en_list = list(range(5702, 7204))\n",
        "vi_list = list(range(14023, 15524))\n",
        "ja_list = list(range(22283, 23785))"
      ],
      "metadata": {
        "id": "Y_kGIg3cBMRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_list = en_list + vi_list + ja_list"
      ],
      "metadata": {
        "id": "21i6aoYuBNUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_list = list(range(4001, 5201))\n",
        "ja_list = list(range(20582, 21783))\n",
        "vi_list = list(range(11322, 13522))"
      ],
      "metadata": {
        "id": "0BF0GOJdBOJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_list = en_list + vi_list + ja_list"
      ],
      "metadata": {
        "id": "z33FPVBeBPTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_list = list(range(0, 23785))\n",
        "train_list = [i for i in full_list if ((i not in test_list) and (i not in val_list))]"
      ],
      "metadata": {
        "id": "W7aR9qBTBQIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkFOFPNna7Nn"
      },
      "outputs": [],
      "source": [
        "test_dataset = train_val_dataset.select(test_list)\n",
        "train_dataset = train_val_dataset.select(train_list)\n",
        "val_dataset = train_val_dataset.select(val_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqEf1cNeXn-u",
        "outputId": "81235722-b7bf-485b-e92d-9c0e099d7fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'question', 'answer', 'image'],\n",
              "    num_rows: 7136\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_val_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Slf-GSZ8atRR",
        "outputId": "7ffd76b7-ac24-4351-dec2-375260b76489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'question', 'answer', 'image'],\n",
              "        num_rows: 12486\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'question', 'answer', 'image'],\n",
              "        num_rows: 4163\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFA71rqbdyHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c6e98f-bbae-484d-d806-0424bc910216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "txt_checkpoint = \"google/mt5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(txt_checkpoint)\n",
        "\n",
        "img_checkpoint = 'facebook/vit-mae-base'\n",
        "image_processor = AutoFeatureExtractor.from_pretrained(img_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q4WYG_zSE6N"
      },
      "outputs": [],
      "source": [
        "class MultimodalCollator:\n",
        "  tokenizer: AutoTokenizer\n",
        "  preprocessor: AutoFeatureExtractor\n",
        "  def __init__(self, tokenizer, preprocessor):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.preprocessor = preprocessor\n",
        "  def tokenize_text(self, texts, targets):\n",
        "\n",
        "      encoded_question = self.tokenizer(\n",
        "          text=texts,\n",
        "          max_length = 80,\n",
        "          padding='max_length',\n",
        "          return_tensors='pt',\n",
        "          return_attention_mask=True,\n",
        "      )\n",
        "      encoded_label = self.tokenizer(\n",
        "          text=targets,\n",
        "          max_length = 80,\n",
        "          padding='max_length',\n",
        "          return_tensors='pt',\n",
        "          return_attention_mask=True,\n",
        "      )\n",
        "      return {\n",
        "          \"input_ids\": encoded_question['input_ids'],\n",
        "          \"labels\": encoded_label['input_ids'],\n",
        "          \"attention_mask\": encoded_question['attention_mask'],\n",
        "          \"decoder_attention_mask\": encoded_label['attention_mask']\n",
        "      }\n",
        "\n",
        "  def preprocess_images(self, images):\n",
        "      processed_images = self.preprocessor(\n",
        "          images,\n",
        "          return_tensors=\"pt\",\n",
        "      )\n",
        "      return {\n",
        "          \"pixel_values\": processed_images['pixel_values'],\n",
        "      }\n",
        "\n",
        "  def __call__(self, examples):\n",
        "      return {\n",
        "          **self.tokenize_text(\n",
        "              examples['question'] if isinstance(examples, dict) else [i['question'] for i in examples],\n",
        "              examples['answer'] if isinstance(examples, dict) else [i['answer'] for i in examples]\n",
        "          ),\n",
        "          **self.preprocess_images(\n",
        "              examples['image'] if isinstance(examples, dict) else [i['image'] for i in examples]\n",
        "          )\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJryfUK2OKBd"
      },
      "outputs": [],
      "source": [
        "collator = MultimodalCollator(tokenizer, image_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rRqxk2I-UYP",
        "outputId": "e72d70d9-54a1-4197-db12-825ff51d7f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/vit-mae-base were not used when initializing ViTMAEModel: ['decoder.decoder_layers.6.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.bias', 'decoder.decoder_layers.5.layernorm_before.bias', 'decoder.decoder_layers.3.attention.attention.query.bias', 'decoder.decoder_layers.3.intermediate.dense.bias', 'decoder.decoder_layers.7.output.dense.bias', 'decoder.decoder_layers.4.layernorm_before.bias', 'decoder.decoder_layers.3.attention.attention.query.weight', 'decoder.decoder_layers.5.attention.attention.query.weight', 'decoder.decoder_layers.1.layernorm_after.bias', 'decoder.decoder_layers.6.layernorm_before.bias', 'decoder.decoder_layers.0.output.dense.bias', 'decoder.decoder_layers.7.attention.output.dense.bias', 'decoder.decoder_layers.6.intermediate.dense.weight', 'decoder.decoder_layers.2.layernorm_before.weight', 'decoder.decoder_layers.3.attention.output.dense.bias', 'decoder.decoder_layers.0.attention.attention.query.bias', 'decoder.decoder_layers.0.attention.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.bias', 'decoder.decoder_layers.4.attention.output.dense.bias', 'decoder.decoder_layers.3.output.dense.weight', 'decoder.decoder_layers.5.layernorm_after.weight', 'decoder.decoder_norm.bias', 'decoder.decoder_layers.6.output.dense.bias', 'decoder.decoder_layers.6.output.dense.weight', 'decoder.decoder_norm.weight', 'decoder.decoder_layers.0.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_before.bias', 'decoder.decoder_layers.5.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.query.weight', 'decoder.decoder_layers.7.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.key.bias', 'decoder.decoder_layers.4.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.query.bias', 'decoder.decoder_layers.3.intermediate.dense.weight', 'decoder.decoder_layers.6.attention.attention.value.weight', 'decoder.decoder_layers.0.layernorm_after.weight', 'decoder.decoder_layers.3.output.dense.bias', 'decoder.decoder_layers.5.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.query.weight', 'decoder.decoder_layers.6.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.query.weight', 'decoder.decoder_layers.3.attention.attention.value.bias', 'decoder.decoder_layers.6.layernorm_after.bias', 'decoder.decoder_layers.2.attention.attention.key.bias', 'decoder.decoder_layers.4.attention.attention.query.bias', 'decoder.decoder_layers.7.layernorm_before.weight', 'decoder.decoder_layers.7.attention.output.dense.weight', 'decoder.decoder_layers.2.output.dense.weight', 'decoder.decoder_layers.2.output.dense.bias', 'decoder.decoder_layers.2.attention.output.dense.weight', 'decoder.decoder_layers.3.layernorm_before.weight', 'decoder.decoder_layers.0.attention.attention.key.bias', 'decoder.decoder_layers.1.layernorm_before.bias', 'decoder.decoder_layers.6.attention.attention.query.weight', 'decoder.decoder_embed.weight', 'decoder.decoder_layers.2.intermediate.dense.bias', 'decoder.decoder_layers.4.layernorm_after.bias', 'decoder.decoder_layers.7.intermediate.dense.bias', 'decoder.decoder_layers.6.attention.attention.key.bias', 'decoder.decoder_layers.3.layernorm_after.weight', 'decoder.decoder_layers.2.layernorm_after.weight', 'decoder.decoder_layers.4.attention.attention.value.bias', 'decoder.decoder_layers.0.attention.output.dense.bias', 'decoder.decoder_layers.5.intermediate.dense.weight', 'decoder.decoder_pred.bias', 'decoder.decoder_layers.1.layernorm_before.weight', 'decoder.decoder_layers.7.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.attention.query.weight', 'decoder.decoder_layers.0.intermediate.dense.weight', 'decoder.decoder_layers.7.attention.attention.query.weight', 'decoder.decoder_layers.2.layernorm_after.bias', 'decoder.decoder_layers.1.attention.attention.query.bias', 'decoder.decoder_layers.5.output.dense.bias', 'decoder.decoder_layers.5.attention.output.dense.weight', 'decoder.decoder_layers.1.layernorm_after.weight', 'decoder.decoder_layers.0.layernorm_before.weight', 'decoder.decoder_layers.5.intermediate.dense.bias', 'decoder.decoder_layers.1.attention.attention.value.weight', 'decoder.decoder_layers.2.attention.attention.key.weight', 'decoder.decoder_layers.1.attention.attention.key.weight', 'decoder.decoder_layers.0.layernorm_after.bias', 'decoder.decoder_layers.3.attention.attention.value.weight', 'decoder.decoder_layers.3.attention.attention.key.weight', 'decoder.mask_token', 'decoder.decoder_layers.0.output.dense.weight', 'decoder.decoder_layers.5.attention.attention.value.bias', 'decoder.decoder_layers.2.layernorm_before.bias', 'decoder.decoder_layers.7.attention.attention.query.bias', 'decoder.decoder_layers.1.output.dense.weight', 'decoder.decoder_embed.bias', 'decoder.decoder_layers.7.output.dense.weight', 'decoder.decoder_layers.4.attention.output.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.bias', 'decoder.decoder_layers.2.intermediate.dense.weight', 'decoder.decoder_layers.4.attention.attention.key.weight', 'decoder.decoder_layers.3.layernorm_before.bias', 'decoder.decoder_layers.2.attention.attention.query.bias', 'decoder.decoder_layers.5.attention.attention.key.bias', 'decoder.decoder_layers.6.attention.attention.value.bias', 'decoder.decoder_layers.1.attention.output.dense.weight', 'decoder.decoder_layers.7.layernorm_before.bias', 'decoder.decoder_layers.7.intermediate.dense.weight', 'decoder.decoder_layers.3.layernorm_after.bias', 'decoder.decoder_pred.weight', 'decoder.decoder_layers.6.layernorm_before.weight', 'decoder.decoder_layers.4.layernorm_before.weight', 'decoder.decoder_layers.4.attention.attention.value.weight', 'decoder.decoder_layers.1.output.dense.bias', 'decoder.decoder_layers.4.intermediate.dense.weight', 'decoder.decoder_layers.1.intermediate.dense.weight', 'decoder.decoder_layers.0.attention.attention.value.bias', 'decoder.decoder_layers.6.attention.attention.query.bias', 'decoder.decoder_layers.7.layernorm_after.weight', 'decoder.decoder_layers.3.attention.output.dense.weight', 'decoder.decoder_layers.4.intermediate.dense.bias', 'decoder.decoder_layers.2.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.value.weight', 'decoder.decoder_layers.7.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.output.dense.bias', 'decoder.decoder_layers.6.attention.output.dense.weight', 'decoder.decoder_layers.6.attention.output.dense.bias', 'decoder.decoder_layers.4.output.dense.bias', 'decoder.decoder_layers.7.attention.attention.key.bias', 'decoder.decoder_layers.0.attention.attention.key.weight', 'decoder.decoder_layers.5.attention.attention.key.weight', 'decoder.decoder_layers.4.attention.attention.key.bias', 'decoder.decoder_layers.1.attention.attention.value.bias', 'decoder.decoder_layers.2.attention.output.dense.bias', 'decoder.decoder_layers.1.attention.attention.key.bias', 'decoder.decoder_layers.6.layernorm_after.weight', 'decoder.decoder_pos_embed', 'decoder.decoder_layers.1.attention.output.dense.bias', 'decoder.decoder_layers.4.layernorm_after.weight', 'decoder.decoder_layers.5.output.dense.weight', 'decoder.decoder_layers.0.intermediate.dense.bias']\n",
            "- This IS expected if you are initializing ViTMAEModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTMAEModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import MT5ForConditionalGeneration, AutoModel\n",
        "image_encoder = AutoModel.from_pretrained(img_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "from transformers.modeling_outputs import Seq2SeqLMOutput, BaseModelOutput\n",
        "\n",
        "class VQA_Model(MT5ForConditionalGeneration):\n",
        "  \"\"\"\n",
        "  The VQAModel should consist of a image encoder and a multilingual language transformer (encoder, decoder, encoder-decoder).\n",
        "\n",
        "  The visual_text_block is a module that integrates the encodings from the images and text.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, config, visualEncoder, img_hidden_dim = 768):\n",
        "    super().__init__(config)\n",
        "    self.visualEncoder = visualEncoder\n",
        "    if img_hidden_dim != config.d_model:\n",
        "      self.needConvert = True\n",
        "      self.dim_change = nn.Linear(img_hidden_dim, config.d_model)\n",
        "\n",
        "  def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        pixel_values: torch.FloatTensor = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
        "        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
        "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ):\n",
        "\n",
        "\n",
        "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "    # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n",
        "    if head_mask is not None and decoder_head_mask is None:\n",
        "        if self.config.num_layers == self.config.num_decoder_layers:\n",
        "            warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n",
        "            decoder_head_mask = head_mask\n",
        "\n",
        "    # Encode if needed (training, first prediction pass)\n",
        "    if encoder_outputs is None:\n",
        "        # Convert encoder inputs in embeddings if needed\n",
        "        text_encoder_outputs = self.encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        image_encoder_outputs = self.visualEncoder(pixel_values = pixel_values)\n",
        "        if self.needConvert:\n",
        "          image_state = self.dim_change(image_encoder_outputs['last_hidden_state'])\n",
        "        else:\n",
        "          image_state = image_encoder_outputs['last_hidden_state']\n",
        "        img_mask = torch.ones((image_state.shape[0], image_state.shape[1])).to(image_state.device)\n",
        "        hidden_states = torch.cat([image_state, text_encoder_outputs[0]], dim = 1)\n",
        "        attention_mask = torch.cat([img_mask, attention_mask,], dim = 1)\n",
        "\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=hidden_states,\n",
        "            hidden_states=text_encoder_outputs[1] if len(text_encoder_outputs) > 1 else None,\n",
        "            attentions=text_encoder_outputs[2] if len(text_encoder_outputs) > 2 else None,\n",
        "        )\n",
        "    elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "        encoder_outputs = BaseModelOutput(\n",
        "            last_hidden_state=encoder_outputs[0],\n",
        "            hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "            attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "        )\n",
        "\n",
        "    if self.model_parallel:\n",
        "        torch.cuda.set_device(self.decoder.first_device)\n",
        "\n",
        "    if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
        "        # get decoder inputs from shifting lm labels to the right\n",
        "        decoder_input_ids = self._shift_right(labels)\n",
        "\n",
        "    # Set device for model parallelism\n",
        "    if self.model_parallel:\n",
        "        torch.cuda.set_device(self.decoder.first_device)\n",
        "        hidden_states = hidden_states.to(self.decoder.first_device)\n",
        "        if decoder_input_ids is not None:\n",
        "            decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(self.decoder.first_device)\n",
        "        if decoder_attention_mask is not None:\n",
        "            decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n",
        "\n",
        "    hidden_states = encoder_outputs[0]\n",
        "\n",
        "\n",
        "    # Decode\n",
        "    decoder_outputs = self.decoder(\n",
        "        input_ids=decoder_input_ids,\n",
        "        attention_mask=decoder_attention_mask,\n",
        "        inputs_embeds=decoder_inputs_embeds,\n",
        "        past_key_values=past_key_values,\n",
        "        encoder_hidden_states=hidden_states,\n",
        "        encoder_attention_mask=attention_mask,\n",
        "        head_mask=decoder_head_mask,\n",
        "        cross_attn_head_mask=cross_attn_head_mask,\n",
        "        use_cache=use_cache,\n",
        "        output_attentions=output_attentions,\n",
        "        output_hidden_states=output_hidden_states,\n",
        "        return_dict=return_dict,\n",
        "    )\n",
        "\n",
        "    sequence_output = decoder_outputs[0]\n",
        "\n",
        "    # Set device for model parallelism\n",
        "    if self.model_parallel:\n",
        "        torch.cuda.set_device(self.encoder.first_device)\n",
        "        self.lm_head = self.lm_head.to(self.encoder.first_device)\n",
        "        sequence_output = sequence_output.to(self.lm_head.weight.device)\n",
        "\n",
        "    if self.config.tie_word_embeddings:\n",
        "        # Rescale output before projecting on vocab\n",
        "        # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
        "        sequence_output = sequence_output * (self.model_dim**-0.5)\n",
        "\n",
        "    lm_logits = self.lm_head(sequence_output)\n",
        "\n",
        "    loss = None\n",
        "    if labels is not None:\n",
        "        loss_fct = nn.CrossEntropyLoss(ignore_index=0)\n",
        "        # move labels to correct device to enable PP\n",
        "        labels = labels.to(lm_logits.device)\n",
        "        loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n",
        "        # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n",
        "\n",
        "    if not return_dict:\n",
        "        output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n",
        "        return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "    return Seq2SeqLMOutput(\n",
        "        loss=loss,\n",
        "        logits=lm_logits,\n",
        "        past_key_values=decoder_outputs.past_key_values,\n",
        "        decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "        decoder_attentions=decoder_outputs.attentions,\n",
        "        cross_attentions=decoder_outputs.cross_attentions,\n",
        "        encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "        encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "        encoder_attentions=encoder_outputs.attentions,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "sL77ANAOOojh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MT5Config\n",
        "model = VQA_Model.from_pretrained(txt_checkpoint, image_encoder)\n",
        "#model = VQA_Model.from_pretrained('G:/Shared drives/CS263_final/models/ViTMAE_mt5/checkpoint-24976')"
      ],
      "metadata": {
        "id": "mSfl8CZvUCQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07620d73-99be-45f9-9ed9-9aa6ff3e1a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of VQA_Model were not initialized from the model checkpoint at google/mt5-small and are newly initialized: ['visualEncoder.encoder.layer.7.attention.attention.query.bias', 'visualEncoder.encoder.layer.5.output.dense.bias', 'visualEncoder.encoder.layer.4.attention.output.dense.weight', 'visualEncoder.encoder.layer.2.layernorm_before.bias', 'visualEncoder.encoder.layer.1.layernorm_after.bias', 'visualEncoder.encoder.layer.2.attention.attention.query.weight', 'visualEncoder.encoder.layer.3.output.dense.bias', 'visualEncoder.encoder.layer.2.attention.attention.query.bias', 'visualEncoder.encoder.layer.5.attention.attention.query.weight', 'visualEncoder.encoder.layer.8.attention.attention.value.weight', 'visualEncoder.encoder.layer.5.output.dense.weight', 'visualEncoder.encoder.layer.11.output.dense.weight', 'visualEncoder.encoder.layer.4.layernorm_after.weight', 'visualEncoder.encoder.layer.0.attention.attention.query.bias', 'visualEncoder.encoder.layer.10.output.dense.weight', 'visualEncoder.encoder.layer.0.attention.attention.query.weight', 'visualEncoder.encoder.layer.4.layernorm_before.weight', 'visualEncoder.encoder.layer.9.layernorm_before.weight', 'visualEncoder.encoder.layer.1.intermediate.dense.weight', 'visualEncoder.encoder.layer.6.layernorm_before.bias', 'visualEncoder.encoder.layer.0.output.dense.bias', 'visualEncoder.encoder.layer.4.layernorm_after.bias', 'visualEncoder.encoder.layer.11.attention.attention.key.weight', 'visualEncoder.encoder.layer.0.attention.attention.value.weight', 'visualEncoder.encoder.layer.8.output.dense.weight', 'visualEncoder.encoder.layer.11.attention.output.dense.bias', 'visualEncoder.encoder.layer.10.layernorm_before.bias', 'visualEncoder.encoder.layer.3.intermediate.dense.bias', 'visualEncoder.encoder.layer.6.output.dense.bias', 'visualEncoder.encoder.layer.1.attention.attention.query.bias', 'visualEncoder.encoder.layer.11.layernorm_after.weight', 'visualEncoder.encoder.layer.10.layernorm_after.weight', 'visualEncoder.encoder.layer.0.output.dense.weight', 'visualEncoder.encoder.layer.7.output.dense.bias', 'visualEncoder.encoder.layer.8.intermediate.dense.weight', 'visualEncoder.encoder.layer.5.layernorm_after.weight', 'visualEncoder.encoder.layer.4.output.dense.weight', 'visualEncoder.encoder.layer.3.attention.attention.query.bias', 'visualEncoder.encoder.layer.5.layernorm_after.bias', 'visualEncoder.encoder.layer.10.attention.attention.query.weight', 'visualEncoder.encoder.layer.1.attention.attention.value.bias', 'visualEncoder.encoder.layer.5.attention.output.dense.bias', 'visualEncoder.encoder.layer.5.attention.attention.query.bias', 'visualEncoder.encoder.layer.0.attention.attention.value.bias', 'visualEncoder.encoder.layer.8.output.dense.bias', 'visualEncoder.encoder.layer.10.attention.attention.query.bias', 'visualEncoder.encoder.layer.2.output.dense.weight', 'visualEncoder.encoder.layer.0.intermediate.dense.bias', 'visualEncoder.encoder.layer.11.attention.attention.value.weight', 'visualEncoder.encoder.layer.5.attention.attention.value.bias', 'visualEncoder.encoder.layer.5.attention.output.dense.weight', 'visualEncoder.encoder.layer.7.attention.attention.query.weight', 'visualEncoder.encoder.layer.9.output.dense.bias', 'visualEncoder.encoder.layer.3.attention.attention.key.weight', 'visualEncoder.encoder.layer.2.attention.attention.key.weight', 'visualEncoder.encoder.layer.2.attention.attention.value.bias', 'visualEncoder.encoder.layer.9.layernorm_after.bias', 'visualEncoder.encoder.layer.11.layernorm_after.bias', 'visualEncoder.encoder.layer.7.output.dense.weight', 'visualEncoder.encoder.layer.7.attention.attention.key.bias', 'visualEncoder.encoder.layer.4.output.dense.bias', 'visualEncoder.encoder.layer.4.attention.attention.value.bias', 'visualEncoder.encoder.layer.6.layernorm_before.weight', 'visualEncoder.encoder.layer.9.attention.output.dense.bias', 'visualEncoder.encoder.layer.2.attention.output.dense.bias', 'visualEncoder.encoder.layer.4.attention.attention.key.bias', 'visualEncoder.encoder.layer.7.attention.attention.value.weight', 'visualEncoder.encoder.layer.0.attention.output.dense.bias', 'visualEncoder.encoder.layer.4.intermediate.dense.bias', 'visualEncoder.encoder.layer.2.attention.attention.value.weight', 'visualEncoder.encoder.layer.10.output.dense.bias', 'visualEncoder.encoder.layer.9.layernorm_after.weight', 'visualEncoder.encoder.layer.2.layernorm_before.weight', 'visualEncoder.encoder.layer.11.attention.output.dense.weight', 'visualEncoder.encoder.layer.3.attention.output.dense.weight', 'visualEncoder.encoder.layer.3.attention.attention.value.bias', 'visualEncoder.encoder.layer.6.attention.attention.value.bias', 'visualEncoder.encoder.layer.0.layernorm_before.weight', 'visualEncoder.encoder.layer.10.intermediate.dense.weight', 'visualEncoder.encoder.layer.6.intermediate.dense.weight', 'visualEncoder.encoder.layer.8.attention.attention.query.bias', 'visualEncoder.encoder.layer.10.layernorm_before.weight', 'visualEncoder.encoder.layer.8.attention.attention.key.bias', 'visualEncoder.encoder.layer.9.output.dense.weight', 'visualEncoder.encoder.layer.5.intermediate.dense.weight', 'visualEncoder.encoder.layer.6.intermediate.dense.bias', 'visualEncoder.encoder.layer.1.output.dense.bias', 'visualEncoder.encoder.layer.4.attention.attention.query.weight', 'visualEncoder.encoder.layer.7.intermediate.dense.bias', 'visualEncoder.encoder.layer.8.layernorm_before.weight', 'visualEncoder.encoder.layer.3.attention.attention.value.weight', 'visualEncoder.encoder.layer.9.attention.attention.key.bias', 'visualEncoder.encoder.layer.6.attention.attention.query.bias', 'visualEncoder.encoder.layer.7.attention.output.dense.bias', 'visualEncoder.encoder.layer.11.attention.attention.query.bias', 'visualEncoder.encoder.layer.10.attention.attention.value.weight', 'visualEncoder.encoder.layer.1.layernorm_after.weight', 'visualEncoder.encoder.layer.11.attention.attention.value.bias', 'visualEncoder.encoder.layer.9.layernorm_before.bias', 'visualEncoder.encoder.layer.1.attention.attention.key.weight', 'visualEncoder.encoder.layer.6.attention.output.dense.weight', 'visualEncoder.encoder.layer.3.layernorm_after.bias', 'visualEncoder.encoder.layer.4.attention.attention.query.bias', 'visualEncoder.encoder.layer.8.attention.output.dense.bias', 'visualEncoder.encoder.layer.3.attention.output.dense.bias', 'visualEncoder.encoder.layer.0.attention.attention.key.bias', 'visualEncoder.encoder.layer.4.attention.output.dense.bias', 'visualEncoder.encoder.layer.11.attention.attention.key.bias', 'visualEncoder.encoder.layer.6.attention.attention.value.weight', 'visualEncoder.encoder.layer.3.attention.attention.key.bias', 'visualEncoder.encoder.layer.10.attention.output.dense.bias', 'visualEncoder.encoder.layer.4.intermediate.dense.weight', 'visualEncoder.encoder.layer.7.attention.output.dense.weight', 'visualEncoder.encoder.layer.1.attention.attention.query.weight', 'visualEncoder.encoder.layer.5.layernorm_before.weight', 'visualEncoder.encoder.layer.6.attention.output.dense.bias', 'visualEncoder.encoder.layer.6.output.dense.weight', 'visualEncoder.encoder.layer.3.output.dense.weight', 'visualEncoder.encoder.layer.7.intermediate.dense.weight', 'visualEncoder.encoder.layer.0.layernorm_after.bias', 'visualEncoder.encoder.layer.8.layernorm_before.bias', 'visualEncoder.encoder.layer.2.layernorm_after.weight', 'visualEncoder.encoder.layer.5.attention.attention.key.weight', 'visualEncoder.encoder.layer.9.attention.attention.query.weight', 'visualEncoder.encoder.layer.10.attention.output.dense.weight', 'visualEncoder.embeddings.patch_embeddings.projection.bias', 'visualEncoder.encoder.layer.2.attention.attention.key.bias', 'dim_change.bias', 'visualEncoder.encoder.layer.9.attention.attention.value.bias', 'visualEncoder.encoder.layer.3.intermediate.dense.weight', 'visualEncoder.encoder.layer.8.attention.output.dense.weight', 'visualEncoder.encoder.layer.11.layernorm_before.bias', 'visualEncoder.encoder.layer.2.attention.output.dense.weight', 'visualEncoder.encoder.layer.1.attention.attention.key.bias', 'visualEncoder.encoder.layer.10.layernorm_after.bias', 'visualEncoder.embeddings.cls_token', 'visualEncoder.encoder.layer.6.layernorm_after.weight', 'visualEncoder.encoder.layer.1.layernorm_before.bias', 'visualEncoder.encoder.layer.6.attention.attention.query.weight', 'visualEncoder.encoder.layer.2.intermediate.dense.bias', 'visualEncoder.encoder.layer.3.layernorm_before.bias', 'visualEncoder.encoder.layer.6.layernorm_after.bias', 'visualEncoder.encoder.layer.11.output.dense.bias', 'visualEncoder.encoder.layer.5.attention.attention.key.bias', 'visualEncoder.embeddings.position_embeddings', 'visualEncoder.layernorm.bias', 'visualEncoder.encoder.layer.10.intermediate.dense.bias', 'visualEncoder.encoder.layer.0.layernorm_after.weight', 'visualEncoder.encoder.layer.3.layernorm_before.weight', 'visualEncoder.embeddings.patch_embeddings.projection.weight', 'visualEncoder.encoder.layer.7.layernorm_before.weight', 'visualEncoder.encoder.layer.2.intermediate.dense.weight', 'visualEncoder.layernorm.weight', 'visualEncoder.encoder.layer.9.intermediate.dense.bias', 'visualEncoder.encoder.layer.11.intermediate.dense.weight', 'visualEncoder.encoder.layer.6.attention.attention.key.weight', 'visualEncoder.encoder.layer.0.attention.output.dense.weight', 'visualEncoder.encoder.layer.4.attention.attention.key.weight', 'visualEncoder.encoder.layer.1.attention.output.dense.bias', 'dim_change.weight', 'visualEncoder.encoder.layer.11.attention.attention.query.weight', 'visualEncoder.encoder.layer.0.intermediate.dense.weight', 'visualEncoder.encoder.layer.8.intermediate.dense.bias', 'visualEncoder.encoder.layer.9.intermediate.dense.weight', 'visualEncoder.encoder.layer.7.attention.attention.value.bias', 'visualEncoder.encoder.layer.5.intermediate.dense.bias', 'visualEncoder.encoder.layer.7.layernorm_after.bias', 'visualEncoder.encoder.layer.1.layernorm_before.weight', 'visualEncoder.encoder.layer.8.attention.attention.value.bias', 'visualEncoder.encoder.layer.10.attention.attention.key.bias', 'visualEncoder.encoder.layer.1.attention.attention.value.weight', 'visualEncoder.encoder.layer.5.attention.attention.value.weight', 'visualEncoder.encoder.layer.2.output.dense.bias', 'visualEncoder.encoder.layer.8.layernorm_after.bias', 'visualEncoder.encoder.layer.6.attention.attention.key.bias', 'visualEncoder.encoder.layer.2.layernorm_after.bias', 'visualEncoder.encoder.layer.3.layernorm_after.weight', 'visualEncoder.encoder.layer.7.layernorm_before.bias', 'visualEncoder.encoder.layer.8.attention.attention.key.weight', 'visualEncoder.encoder.layer.1.attention.output.dense.weight', 'visualEncoder.encoder.layer.4.attention.attention.value.weight', 'visualEncoder.encoder.layer.8.layernorm_after.weight', 'visualEncoder.encoder.layer.11.layernorm_before.weight', 'visualEncoder.encoder.layer.10.attention.attention.key.weight', 'visualEncoder.encoder.layer.10.attention.attention.value.bias', 'visualEncoder.encoder.layer.0.layernorm_before.bias', 'visualEncoder.encoder.layer.9.attention.output.dense.weight', 'visualEncoder.encoder.layer.1.output.dense.weight', 'visualEncoder.encoder.layer.5.layernorm_before.bias', 'visualEncoder.encoder.layer.4.layernorm_before.bias', 'visualEncoder.encoder.layer.7.attention.attention.key.weight', 'visualEncoder.encoder.layer.3.attention.attention.query.weight', 'visualEncoder.encoder.layer.9.attention.attention.value.weight', 'visualEncoder.encoder.layer.8.attention.attention.query.weight', 'visualEncoder.encoder.layer.0.attention.attention.key.weight', 'visualEncoder.encoder.layer.11.intermediate.dense.bias', 'visualEncoder.encoder.layer.1.intermediate.dense.bias', 'visualEncoder.encoder.layer.9.attention.attention.key.weight', 'visualEncoder.encoder.layer.7.layernorm_after.weight', 'visualEncoder.encoder.layer.9.attention.attention.query.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.generation_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iiSLrS1JyMR",
        "outputId": "e7e24dfe-0f16-4790-bc66-a2450f9594d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"_from_model_config\": true,\n",
              "  \"decoder_start_token_id\": 0,\n",
              "  \"eos_token_id\": 1,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"transformers_version\": \"4.29.2\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the image and text encoders\n",
        "for param in model.shared.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for param in model.visualEncoder.parameters():\n",
        "  param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "f67wtm5FPjUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, GenerationConfig\n",
        "\n",
        "if os.getcwd() == '/content':\n",
        "  save_location = '/content/drive/Shareddrives/CS263_final/models/ViTMAE_mt5/'\n",
        "else:\n",
        "  save_location = 'G:/Shared drives/CS263_final/models/ViTMAE_mt5/'\n",
        "\n",
        "lr = 3e-4\n",
        "train_args = Seq2SeqTrainingArguments(\n",
        "    output_dir = save_location,\n",
        "    overwrite_output_dir = True,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate = lr,\n",
        "    num_train_epochs = 4,\n",
        "    logging_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    seed = 2023,\n",
        "    fp16 = False,\n",
        "    bf16 = True,\n",
        "    load_best_model_at_end = True,\n",
        "    generation_config = GenerationConfig.from_pretrained(txt_checkpoint)\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=400,\n",
        "                                               gamma=0.9)\n",
        "\n",
        "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
        "  def get_train_dataloader(self):\n",
        "    # build train dataloader\n",
        "    train_loader = DataLoader(self.train_dataset, batch_size=self.args.per_device_train_batch_size, shuffle = True, num_workers = 0, collate_fn = collator)\n",
        "    return train_loader\n",
        "  def get_eval_dataloader(self, eval_dataset):\n",
        "    eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "    eval_loader = DataLoader(eval_dataset, batch_size=self.args.per_device_eval_batch_size, shuffle = True, num_workers = 0, collate_fn = collator)\n",
        "    return eval_loader\n",
        "  def get_test_dataloader(self, test_dataset):\n",
        "    test_dataset = test_dataset if test_dataset is not None else self.test_dataset\n",
        "    test_loader = DataLoader(test_dataset, batch_size=self.args.per_device_eval_batch_size, shuffle = True, num_workers = 0, collate_fn = collator)\n",
        "    return test_loader\n",
        "\n",
        "def compute_metric(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis = -1)\n",
        "  return metric.compute(predictions, references = labels)\n",
        "\n",
        "trainer = CustomSeq2SeqTrainer(\n",
        "                  model,\n",
        "                  args = train_args,\n",
        "                  data_collator = collator,\n",
        "                  train_dataset = train_dataset,\n",
        "                  eval_dataset = val_dataset,\n",
        "                  optimizers = (optimizer, scheduler),\n",
        "                  # compute_metrics=compute_metric\n",
        "                 )\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_XLRwEnTxgB",
        "outputId": "4fc6a4bb-889f-444e-f608-b3fed7a9e8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VQA_Model(\n",
              "  (shared): Embedding(250112, 512)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
              "  (visualEncoder): ViTMAEModel(\n",
              "    (embeddings): ViTMAEEmbeddings(\n",
              "      (patch_embeddings): ViTMAEPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "    )\n",
              "    (encoder): ViTMAEEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTMAELayer(\n",
              "          (attention): ViTMAEAttention(\n",
              "            (attention): ViTMAESelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTMAESelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTMAEIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTMAEOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (dim_change): Linear(in_features=768, out_features=512, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "EFqlNkducyRG",
        "outputId": "49e10fe0-ecca-4959-c1ee-0fcfb3bb7170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14680' max='14680' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14680/14680 46:16, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.297400</td>\n",
              "      <td>1.963122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.244200</td>\n",
              "      <td>1.854659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.083100</td>\n",
              "      <td>1.836426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.026000</td>\n",
              "      <td>1.828433</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=14680, training_loss=2.4127052244763285, metrics={'train_runtime': 2778.927, 'train_samples_per_second': 21.129, 'train_steps_per_second': 5.283, 'total_flos': 7280166514360320.0, 'train_loss': 2.4127052244763285, 'epoch': 4.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.lr_scheduler.get_last_lr()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc-29MuPUktI",
        "outputId": "bbd3df6c-689b-4cfa-bad2-62dbd63b89f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.758519863481758e-06]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "from transformers import GenerationConfig\n",
        "config = GenerationConfig.from_pretrained(txt_checkpoint, max_length = 80)\n",
        "data = collator(test_dataset[:4])\n",
        "for k, v in data.items():\n",
        "  data[k] = v.to(device)\n",
        "labels = data.pop('labels' )\n",
        "data.pop(\"decoder_attention_mask\")\n",
        "generation_output = model.generate(**data, generation_config = config, return_dict_in_generate = True, output_scores = True, temperature = 2 )"
      ],
      "metadata": {
        "id": "a3qdBqF9ftrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f403d2ce-c015-4244-d90e-8beec7c94938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (80) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(generation_output['sequences'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKRtCuKG_RXZ",
        "outputId": "defeeb72-730f-4d4d-be74-16e4945840e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad> there are two pillars in front of thetower</s><pad>',\n",
              " '<pad> the green scale is green</s><pad><pad><pad><pad><pad><pad><pad>',\n",
              " '<pad> the wall is hanging on the wall in the room</s><pad>',\n",
              " '<pad> the households are hang on the side of the road</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[:4]['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWDkRRvqOSNM",
        "outputId": "08d720df-f87e-49f4-cba4-2a0ed7a8bbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['there are two pillars in front of the tower',\n",
              " 'in front of the woman hunching over hher back',\n",
              " 'the drawings',\n",
              " 'the red flag with yellow star']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[:4]['question']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0Vii0LgPG7R",
        "outputId": "df3557a6-e4b4-47e8-8998-e25928469931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['how many pillars are there in front of the tower?',\n",
              " 'where is the green scale?',\n",
              " 'what is hanging on the wall in the room?',\n",
              " 'what flags do the households here hang on the side of the road?']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output = model.generate(**data, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 4 )"
      ],
      "metadata": {
        "id": "vkZ1QUDEQNJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.batch_decode(generation_output['sequences'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiznhz6p_rIH",
        "outputId": "f9ff803a-eba8-4201-eb40-774c855e7f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad> there are two pillars in front of thetower</s><pad>',\n",
              " '<pad> the green scale is green</s><pad><pad><pad><pad><pad><pad><pad>',\n",
              " '<pad> the wall is hanging on the wall in the room</s><pad>',\n",
              " '<pad> the households are hang on the side of the road</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output['sequences']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmtDk9aM8X-x",
        "outputId": "a28a17cb-b4dd-4d7a-82a1-bf67f717fa3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0, 77919,     1,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0],\n",
              "        [    0,  1672,   259,   272,  2832,   282,  3325,   259,   272,  3456,\n",
              "             1]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score"
      ],
      "metadata": {
        "id": "eAl1KI9hwLN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = F1Score(task = 'multiclass', num_classes = 250112, top_k=1, ignore_index=0)"
      ],
      "metadata": {
        "id": "wrypLHOLRvME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score\n",
        "from langdetect import detect\n",
        "greedy_F1_scores = []\n",
        "for data in test_dataset:\n",
        "  lang = detect(data['question'])\n",
        "  inputs = collator(data)\n",
        "  for k, v in inputs.items():\n",
        "    inputs[k] = v.to(device)\n",
        "  labels = inputs.pop('labels' )\n",
        "  inputs.pop(\"decoder_attention_mask\")\n",
        "  generation_output = model.generate(**inputs, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 1)\n",
        "  pred_str = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens = True)\n",
        "  padded_str = tokenizer(pred_str +  [data['answer']], padding = 'longest', return_attention_mask = False, return_tensors='pt')\n",
        "  padded_pred_token = padded_str['input_ids'][0]\n",
        "  padded_label_token = padded_str['input_ids'][1]\n",
        "  score = f1(padded_pred_token, padded_label_token)\n",
        "  greedy_F1_scores.append(score)\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "greedy_f1_score_avg = np.mean(greedy_F1_scores)"
      ],
      "metadata": {
        "id": "j2-_upmMgzo0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b223ff79-80ed-483d-a2ff-c4604d5078f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (80) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_f1_score_avg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOshbtc4lpEm",
        "outputId": "2ee75fbc-a181-4fbf-8d08-cb09a985eba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19819523"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score\n",
        "beam4_F1_scores = []\n",
        "for data in test_dataset:\n",
        "  inputs = collator(data)\n",
        "  for k, v in inputs.items():\n",
        "    inputs[k] = v.to(device)\n",
        "  labels = inputs.pop('labels' )\n",
        "  inputs.pop(\"decoder_attention_mask\")\n",
        "  generation_output = model.generate(**inputs, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 4)\n",
        "  pred_str = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens = True)\n",
        "  padded_str = tokenizer(pred_str +  [data['answer']], padding = 'longest', return_attention_mask = False, return_tensors='pt')\n",
        "  padded_pred_token = padded_str['input_ids'][0]\n",
        "  padded_label_token = padded_str['input_ids'][1]\n",
        "  score = f1(padded_pred_token, padded_label_token)\n",
        "  beam4_F1_scores.append(score)\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "beam4_f1_score_avg = np.mean(beam4_F1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDgkgbdbae0Q",
        "outputId": "c727bda0-f86d-45ee-cf74-d66014312e11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (80) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam4_f1_score_avg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk7VAOFh4gDA",
        "outputId": "636ff9e2-bf44-44a5-bb17-b6a940280fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20050485"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffled_set = test_dataset.shuffle(seed = 2023).select(list(range(1000)))\n",
        "beam4_F1_scores = []\n",
        "for data in shuffled_set:\n",
        "  inputs = collator(data)\n",
        "  for k, v in inputs.items():\n",
        "    inputs[k] = v.to(device)\n",
        "  labels = inputs.pop('labels' )\n",
        "  inputs.pop(\"decoder_attention_mask\")\n",
        "  generation_output = model.generate(**inputs, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 4)\n",
        "  pred_str = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens = True)\n",
        "  padded_str = tokenizer(pred_str +  [data['answer']], padding = 'longest', return_attention_mask = False, return_tensors='pt')\n",
        "  padded_pred_token = padded_str['input_ids'][0]\n",
        "  padded_label_token = padded_str['input_ids'][1]\n",
        "  score = f1(padded_pred_token, padded_label_token)\n",
        "  beam4_F1_scores.append(score)\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "beam4_f1_score_avg = np.mean(beam4_F1_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ww7PbRF-Stq9",
        "outputId": "49f5a156-70e8-490d-f239-9b6292c84275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached shuffled indices for dataset at G:\\Shared drives\\CS263_final\\models\\data\\evjvqa_train_PIL_image\\cache-ccdcb774d819f7c1.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beam4_f1_score_avg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aauKVjJbEWPb",
        "outputId": "3e7f248d-7757-45c6-ecd2-afa1a232a787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19243912"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import string\n",
        "from nltk.translate import bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from langdetect import detect\n",
        "smoothie = SmoothingFunction().method4\n",
        "def bleu_score(reference, candidate):\n",
        "  # ignore punctuations\n",
        "  reference = reference.translate(str.maketrans('', '', string.punctuation))\n",
        "  candidate = candidate.translate(str.maketrans('', '', string.punctuation))\n",
        "  ref_list, can_list = [],[]\n",
        "  # convert string to list of words\n",
        "  try:\n",
        "      lang = detect(reference)\n",
        "  except:\n",
        "      lang = \"error\"\n",
        "      print(\"error for detecting: \", reference)\n",
        "  # Detect if the language is Japanese, whose words in answers are not split by space.\n",
        "  # Need to make it a list of characters for bleu score calculation.\n",
        "  if lang != \"en\" or lang != \"vi\":\n",
        "    # ignore possible spaces in Japanese sentences\n",
        "    reference = reference.replace(\" \", \"\")\n",
        "    candidate = candidate.replace(\" \", \"\")\n",
        "    for ch in reference:\n",
        "      ref_list.append(ch)\n",
        "    for ch in candidate:\n",
        "      can_list.append(ch)\n",
        "  else:\n",
        "    ref_list = reference.split()\n",
        "    can_list = candidate.split()\n",
        "\n",
        "  score = bleu([ref_list], can_list,smoothing_function=smoothie, weights = [0.25,0.25,0.25,0.25])\n",
        "  return score\n",
        "\n",
        "# Evaluate model performance on test dataset\n",
        "def test_eval(test_data, model):\n",
        "  f1_torchmetric, bleu_l, pred_token_l, pred_word_l = [],[],[],[]\n",
        "  f1_en, f1_ja, f1_vi = [],[],[]\n",
        "  bleu_en, bleu_ja, bleu_vi = [],[],[]\n",
        "  f1 = F1Score(task=\"multiclass\", num_classes=250, topk = 1, ignore_index = 0)\n",
        "  for data in test_data:\n",
        "    inputs = collator(data)\n",
        "    for k, v in inputs.items():\n",
        "      inputs[k] = v.to(device)\n",
        "\n",
        "    labels = inputs.pop('labels' )\n",
        "    inputs.pop(\"decoder_attention_mask\")\n",
        "    generation_output = model.generate(**inputs, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 4)\n",
        "    pred_str = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens = True)\n",
        "    padded_str = tokenizer(pred_str +  [data['answer']], padding = 'longest', return_attention_mask = False, return_tensors='pt')\n",
        "    padded_pred_token = padded_str['input_ids'][0]\n",
        "    padded_label_token = padded_str['input_ids'][1]\n",
        "\n",
        "    # F1 score from torch.metric\n",
        "\n",
        "    s3 = f1(padded_pred_token, padded_label_token)\n",
        "    f1_torchmetric.append(s3)\n",
        "\n",
        "    # Compute bleu score\n",
        "    s2 = bleu_score(data['answer'], pred_str[0])\n",
        "    bleu_l.append(s2)\n",
        "\n",
        "    try:\n",
        "      lang = detect(data['answer'])\n",
        "    except:\n",
        "      lang = \"error\"\n",
        "      #print(\"error for detecting: \", answer)\n",
        "    if lang == \"en\":\n",
        "      f1_en.append(s3)\n",
        "      bleu_en.append(s2)\n",
        "    elif lang == \"vi\":\n",
        "      f1_vi.append(s3)\n",
        "      bleu_vi.append(s2)\n",
        "    else:\n",
        "      f1_ja.append(s3)\n",
        "      bleu_ja.append(s2)\n",
        "\n",
        "\n",
        "  return f1_torchmetric, bleu_l, pred_token_l, pred_word_l, f1_en, f1_ja, f1_vi, bleu_en, bleu_ja, bleu_vi"
      ],
      "metadata": {
        "id": "r7NpnnWxR6SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72e8241-f38e-43ff-8ca5-309c7548ef65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "f1_torchmetric, bleu_l, pred_token_l, pred_word_l, f1_en, f1_ja, f1_vi, bleu_en, bleu_ja, bleu_vi = test_eval(shuffled_set,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6YKEFTQ45xF",
        "outputId": "bb2d560e-02f3-4637-82b9-b9e0d723a9eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (80) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"average f1 score: \", sum(f1_torchmetric)/len(f1_torchmetric))\n",
        "print(\"average f1 score in en: \", sum(f1_en)/len(f1_en))\n",
        "print(\"average f1 score in ja: \", sum(f1_ja)/len(f1_ja))\n",
        "print(\"average f1 score in vi: \", sum(f1_vi)/len(f1_vi))\n",
        "print(\"average bleu score: \", sum(bleu_l)/len(bleu_l))\n",
        "print(\"average bleu score in en: \", sum(bleu_en)/len(bleu_en))\n",
        "print(\"average bleu score in ja: \", sum(bleu_ja)/len(bleu_ja))\n",
        "print(\"average bleu score in vi: \", sum(bleu_vi)/len(bleu_vi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hSltyRQNkVl",
        "outputId": "83c545a8-db1e-4027-be3f-efe9250ab229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "average f1 score:  tensor(0.1924)\n",
            "average f1 score in en:  tensor(0.2036)\n",
            "average f1 score in ja:  tensor(0.1932)\n",
            "average f1 score in vi:  tensor(0.1867)\n",
            "average bleu score:  0.2650023270519166\n",
            "average bleu score in en:  0.27667069040263736\n",
            "average bleu score in ja:  0.1830267181497897\n",
            "average bleu score in vi:  0.3152150600395817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_output['sequences']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywzhIdHW5HNe",
        "outputId": "4ba042f2-7a0e-4454-b940-c5564127e12a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[     0,    259, 237638, 160699, 145710,    535, 215851,    306,      1]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annot_path = 'G:\\Shared drives\\CS263_final\\models\\data\\official_evjvqa_public_test.json'\n",
        "img_zip_path = 'G:\\Shared drives\\CS263_final\\models\\data\\public-test-images.zip'\n",
        "with open(annot_path, encoding = 'utf-8') as f:\n",
        "  \"\"\"\n",
        "  Data file structure:\n",
        "  {\n",
        "    images: {\n",
        "      'id': image_id\n",
        "      'filename': reference file\n",
        "    },\n",
        "    'annotations': {\n",
        "      'id': annotation_id\n",
        "      'image_id': refers to id in images\n",
        "      'question': question about image\n",
        "      'answer': answer to question\n",
        "    }\n",
        "  }\n",
        "  \"\"\"\n",
        "  train_data = json.load(f)\n",
        "  img_reference = pd.DataFrame(train_data['images']).set_index('id')"
      ],
      "metadata": {
        "id": "km0piEBBR7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EVJVQA_Dataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  Dataset class for the EVJVQA dataset.\n",
        "  \"\"\"\n",
        "  def __init__(self, annotation_file, img_dir, zip_subpath):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      annotation_file - file path for the annotation json file\n",
        "      img_dir - file path for the image zip file\n",
        "      zip_subpath - subfolder in the zip folder\n",
        "    \"\"\"\n",
        "    with open(annotation_file, encoding = 'utf-8') as f:\n",
        "      json_file = json.load(f)\n",
        "    self.annotations = pd.DataFrame(json_file['annotations'])\n",
        "    self.img_reference = pd.DataFrame(json_file['images']).set_index('id')\n",
        "    self.img_dir = img_dir\n",
        "    self.transform = ToTensor()\n",
        "    self.subpath = zip_subpath + '/'\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    annot_id = self.annotations.loc[idx, 'id']\n",
        "    image_id = self.annotations.loc[idx, 'image_id']\n",
        "    question = self.annotations.loc[idx, 'question']\n",
        "    answer = self.annotations.loc[idx, 'answer']\n",
        "    img_file = self.img_reference.loc[image_id, 'filename']\n",
        "    with zipfile.ZipFile(self.img_dir, 'r') as zip_ref:\n",
        "       imgdata = zip_ref.open(self.subpath + img_file)\n",
        "       img = Image.open(imgdata).convert('RGB')\n",
        "    img = self.transform(img)\n",
        "    return {\n",
        "        'id': annot_id,\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'image': img,\n",
        "    }"
      ],
      "metadata": {
        "id": "KtpsOnNutD6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = EVJVQA_Dataset(annot_path, img_zip_path, 'public-test-images')"
      ],
      "metadata": {
        "id": "DZEZs8mpSLPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(dataset, collate_fn = collator)"
      ],
      "metadata": {
        "id": "h86GxZF_u5aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for i in dataset:\n",
        "  id = i['id']\n",
        "  inputs = collator(i)\n",
        "  for k, v in inputs.items():\n",
        "    inputs[k] = v.to(device)\n",
        "  labels = inputs.pop('labels' )\n",
        "  inputs.pop(\"decoder_attention_mask\")\n",
        "  generation_output = model.generate(**inputs, generation_config = config, return_dict_in_generate = True, output_scores = True, num_beams = 4)\n",
        "  pred_str = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens = True)\n",
        "  results[str(id)] = pred_str[0]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EeRHsYFMv5hd",
        "outputId": "3a196865-8953-4e6a-add0-ce219ec32765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\josep\\anaconda3\\envs\\pt\\lib\\site-packages\\transformers\\generation\\utils.py:1346: UserWarning: Using `max_length`'s default (80) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[1;32m~\\anaconda3\\envs\\pt\\lib\\site-packages\\pandas\\core\\indexes\\range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[1;31mValueError\u001b[0m: 5015 is not in range",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[169], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m collator(i)\n",
            "Cell \u001b[1;32mIn[160], line 24\u001b[0m, in \u001b[0;36mEVJVQA_Dataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 24\u001b[0m   annot_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     25\u001b[0m   image_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     26\u001b[0m   question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\pt\\lib\\site-packages\\pandas\\core\\indexing.py:1066\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\pt\\lib\\site-packages\\pandas\\core\\frame.py:3924\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3918\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   3920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3921\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3922\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3923\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 3924\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   3927\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   3928\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3\\envs\\pt\\lib\\site-packages\\pandas\\core\\indexes\\range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 5015"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('G:/Shared drives/CS263_final/public_results.json', 'w') as f:\n",
        "  json.dump(results, f)"
      ],
      "metadata": {
        "id": "XkHTpJLlwCOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90AfcBRg5ANu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}